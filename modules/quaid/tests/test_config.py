"""Tests for config.py â€” camelCase conversion, dataclasses, config loading."""

import os
import sys
import json
import sqlite3
import tempfile
import threading
import time
from pathlib import Path
from unittest.mock import patch

# Ensure plugin root is on the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

# Set env to avoid touching real DB
os.environ.setdefault("MEMORY_DB_PATH", ":memory:")

import pytest

from config import (
    _camel_to_snake,
    _load_nested,
    ModelConfig,
    MemoryConfig,
    load_config,
    get_config,
    reload_config,
    _config_paths,
)


# ---------------------------------------------------------------------------
# _camel_to_snake
# ---------------------------------------------------------------------------

class TestCamelToSnake:
    """Tests for _camel_to_snake() conversion."""

    def test_basic_camel_case(self):
        assert _camel_to_snake("batchBudgetPercent") == "batch_budget_percent"

    def test_fast_reasoning(self):
        assert _camel_to_snake("fastReasoning") == "fast_reasoning"

    def test_already_snake(self):
        assert _camel_to_snake("already_snake") == "already_snake"

    def test_all_caps(self):
        assert _camel_to_snake("ABC") == "a_b_c"

    def test_single_word(self):
        assert _camel_to_snake("model") == "model"

    def test_leading_lowercase(self):
        assert _camel_to_snake("maxTokens") == "max_tokens"

    def test_consecutive_caps(self):
        assert _camel_to_snake("HTMLParser") == "h_t_m_l_parser"

    def test_empty_string(self):
        assert _camel_to_snake("") == ""

    def test_single_char(self):
        assert _camel_to_snake("a") == "a"

    def test_numbers_unchanged(self):
        assert _camel_to_snake("model4Name") == "model4_name"


# ---------------------------------------------------------------------------
# _load_nested
# ---------------------------------------------------------------------------

class TestLoadNested:
    """Tests for _load_nested() recursive key conversion."""

    def test_simple_dict(self):
        result = _load_nested({"camelCase": "value"})
        assert result == {"camel_case": "value"}

    def test_nested_dict(self):
        result = _load_nested({"outer": {"innerKey": "value"}})
        assert result == {"outer": {"inner_key": "value"}}

    def test_preserves_lists(self):
        result = _load_nested({"myList": [1, 2, 3]})
        assert result == {"my_list": [1, 2, 3]}

    def test_empty_dict(self):
        assert _load_nested({}) == {}


# ---------------------------------------------------------------------------
# ModelConfig
# ---------------------------------------------------------------------------

class TestModelConfig:
    """Tests for ModelConfig dataclass."""

    def test_context_window_high(self):
        cfg = ModelConfig(deep_reasoning_context=200000)
        assert cfg.context_window("deep") == 200000

    def test_context_window_low(self):
        cfg = ModelConfig(fast_reasoning_context=150000)
        assert cfg.context_window("fast") == 150000

    def test_context_window_unknown_tier_defaults_to_low(self):
        cfg = ModelConfig(fast_reasoning_context=100000)
        assert cfg.context_window("unknown") == 100000

    def test_default_values(self):
        cfg = ModelConfig()
        assert cfg.fast_reasoning == "default"
        assert cfg.deep_reasoning == "default"
        assert cfg.batch_budget_percent == 0.50

    def test_custom_values(self):
        cfg = ModelConfig(
            fast_reasoning="custom-model",
            deep_reasoning_context=500000,
            batch_budget_percent=0.30,
        )
        assert cfg.fast_reasoning == "custom-model"
        assert cfg.deep_reasoning_context == 500000
        assert cfg.batch_budget_percent == 0.30


# ---------------------------------------------------------------------------
# Config loading and caching
# ---------------------------------------------------------------------------

class TestConfigLoading:
    """Tests for config loading and caching."""

    def test_default_config_when_no_file(self):
        """When CONFIG_PATHS all point to nonexistent files, defaults are used."""
        import config
        old_config = config._config
        config._config = None
        try:
            with patch.object(config, "_config_paths", lambda: [Path("/nonexistent/path.json")]):
                cfg = load_config()
                assert isinstance(cfg, MemoryConfig)
                # Verify defaults
                assert cfg.decay.threshold_days == 30
                assert cfg.retrieval.default_limit == 5
                assert cfg.prompt_set == "default"
        finally:
            config._config = old_config

    def test_get_config_caching(self):
        """get_config() returns the same object on repeated calls."""
        import config
        old_config = config._config
        config._config = None
        try:
            with patch.object(config, "_config_paths", lambda: [Path("/nonexistent/path.json")]):
                cfg1 = get_config()
                cfg2 = get_config()
                assert cfg1 is cfg2
        finally:
            config._config = old_config

    def test_reload_config_clears_cache(self):
        """reload_config() returns a fresh object."""
        import config
        old_config = config._config
        config._config = None
        try:
            with patch.object(config, "_config_paths", lambda: [Path("/nonexistent/path.json")]):
                cfg1 = get_config()
                cfg2 = reload_config()
                # After reload, should be a different object
                assert cfg1 is not cfg2
        finally:
            config._config = old_config

    def test_reload_config_resets_loading_guard_before_rebuild(self):
        import config

        old_config = config._config
        old_loading = config._config_loading
        config._config = None
        config._config_loading = True
        try:
            calls = {"n": 0}

            def _fake_inner():
                calls["n"] += 1
                cfg = MemoryConfig()
                config._config = cfg
                return cfg

            with patch.object(config, "_load_config_inner", side_effect=_fake_inner):
                cfg = reload_config()
            assert isinstance(cfg, MemoryConfig)
            assert calls["n"] == 1
        finally:
            config._config = old_config
            config._config_loading = old_loading

    def test_load_config_thread_safe_singleton(self):
        import config

        old_config = config._config
        old_loading = config._config_loading
        config._config = None
        config._config_loading = False
        try:
            calls = {"n": 0}

            def _slow_inner():
                calls["n"] += 1
                time.sleep(0.05)
                cfg = MemoryConfig()
                config._config = cfg
                return cfg

            results = []
            errors = []

            def _worker():
                try:
                    results.append(load_config())
                except Exception as exc:
                    errors.append(exc)

            with patch.object(config, "_load_config_inner", side_effect=_slow_inner):
                threads = [threading.Thread(target=_worker) for _ in range(6)]
                for t in threads:
                    t.start()
                for t in threads:
                    t.join()

            assert not errors
            assert calls["n"] == 1
            assert len(results) == 6
            first = results[0]
            assert all(r is first for r in results)
        finally:
            config._config = old_config
            config._config_loading = old_loading

    def test_load_from_json_file(self, tmp_path):
        """Config can be loaded from a JSON file."""
        import config
        old_config = config._config
        config._config = None
        try:
            config_data = {
                "decay": {"thresholdDays": 60},
                "retrieval": {"defaultLimit": 10}
            }
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps(config_data))

            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                assert cfg.adapter.type == "standalone"
                assert cfg.decay.threshold_days == 60
                assert cfg.retrieval.default_limit == 10
        finally:
            config._config = old_config

    def test_loads_prompt_set_from_config(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({"promptSet": "default"}))
            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                assert cfg.prompt_set == "default"
        finally:
            config._config = old_config

    def test_warns_on_unknown_keys(self, tmp_path, capsys):
        import config
        old_config = config._config
        old_warned = set(config._warned_unknown_config_keys)
        config._config = None
        config._warned_unknown_config_keys.clear()
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({
                "models": {"llmProvider": "default", "llmProvderTypo": "bad"},
                "totallyUnknownSection": {"enabled": True},
            }))
            with patch.object(config, "_config_paths", lambda: [config_file]), \
                 patch.dict(os.environ, {"QUAID_QUIET": ""}, clear=False):
                _ = load_config()
            err = capsys.readouterr().err
            assert "Unknown config key ignored: models.llm_provder_typo" in err
            assert "Unknown config key ignored: totally_unknown_section" in err
        finally:
            config._config = old_config
            config._warned_unknown_config_keys.clear()
            config._warned_unknown_config_keys.update(old_warned)

    def test_loads_identity_and_privacy_blocks(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({
                "identity": {
                    "mode": "multi_user",
                    "autoLinkThreshold": 0.91,
                    "requireReviewThreshold": 0.62
                },
                "privacy": {
                    "defaultScopeDm": "private_subject",
                    "defaultScopeGroup": "source_shared",
                    "enforceStrictFilters": True
                }
            }))
            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                assert cfg.identity.mode == "multi_user"
                assert cfg.identity.auto_link_threshold == 0.91
                assert cfg.identity.require_review_threshold == 0.62
                assert cfg.privacy.default_scope_dm == "private_subject"
                assert cfg.privacy.default_scope_group == "source_shared"
                assert cfg.privacy.enforce_strict_filters is True
        finally:
            config._config = old_config

    def test_loads_janitor_token_budget_from_config(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_data = {
                "janitor": {"tokenBudget": 12345}
            }
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps(config_data))

            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                assert cfg.janitor.token_budget == 12345
        finally:
            config._config = old_config

    def test_loads_adapter_type_from_config(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({"adapter": {"type": "openclaw"}}))
            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                assert cfg.adapter.type == "openclaw"
        finally:
            config._config = old_config

    def test_loads_janitor_apply_mode_from_config(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({"janitor": {"applyMode": "ask"}}))
            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                assert cfg.janitor.apply_mode == "ask"
        finally:
            config._config = old_config

    def test_loads_plugins_config_block(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            (tmp_path / "plugins" / "openclaw").mkdir(parents=True)
            (tmp_path / "plugins" / "openclaw" / "plugin.json").write_text(json.dumps({
                "plugin_api_version": 1,
                "plugin_id": "openclaw.adapter",
                "plugin_type": "adapter",
                "module": "adaptors.openclaw.adapter",
                "capabilities": {
                    "display_name": "OpenClaw Adapter",
                    "contract": {
                        "init": {"mode": "hook", "handler": "on_init"},
                        "config": {"mode": "hook", "handler": "on_config"},
                        "status": {"mode": "hook"},
                        "dashboard": {"mode": "tbd"},
                        "maintenance": {"mode": "hook"},
                        "tool_runtime": {"mode": "hook", "handler": "on_tool_runtime"},
                        "health": {"mode": "hook"},
                        "tools": {"mode": "declared", "exports": []},
                        "api": {"mode": "declared", "exports": []},
                        "events": {"mode": "declared", "exports": []},
                        "ingest_triggers": {"mode": "declared", "exports": []},
                        "auth_requirements": {"mode": "declared", "exports": []},
                        "migrations": {"mode": "declared", "exports": []},
                        "notifications": {"mode": "declared", "exports": []},
                    },
                },
            }))
            (tmp_path / "vendor" / "plugins" / "core-extract").mkdir(parents=True)
            (tmp_path / "vendor" / "plugins" / "core-extract" / "plugin.json").write_text(json.dumps({
                "plugin_api_version": 1,
                "plugin_id": "core.extract",
                "plugin_type": "ingest",
                "module": "ingest.core",
                "capabilities": {
                    "display_name": "Core Extract Ingest",
                    "contract": {
                        "init": {"mode": "hook", "handler": "on_init"},
                        "config": {"mode": "hook", "handler": "on_config"},
                        "status": {"mode": "hook"},
                        "dashboard": {"mode": "tbd"},
                        "maintenance": {"mode": "hook"},
                        "tool_runtime": {"mode": "hook", "handler": "on_tool_runtime"},
                        "health": {"mode": "hook"},
                        "tools": {"mode": "declared", "exports": []},
                        "api": {"mode": "declared", "exports": []},
                        "events": {"mode": "declared", "exports": []},
                        "ingest_triggers": {"mode": "declared", "exports": []},
                        "auth_requirements": {"mode": "declared", "exports": []},
                        "migrations": {"mode": "declared", "exports": []},
                        "notifications": {"mode": "declared", "exports": []},
                    },
                },
            }))
            (tmp_path / "vendor" / "plugins" / "memorydb").mkdir(parents=True)
            (tmp_path / "vendor" / "plugins" / "memorydb" / "plugin.json").write_text(json.dumps({
                "plugin_api_version": 1,
                "plugin_id": "memorydb.core",
                "plugin_type": "datastore",
                "module": "datastore.memorydb",
                "capabilities": {
                    "display_name": "MemoryDB",
                    "contract": {
                        "init": {"mode": "hook", "handler": "on_init"},
                        "config": {"mode": "hook", "handler": "on_config"},
                        "status": {"mode": "hook"},
                        "dashboard": {"mode": "tbd"},
                        "maintenance": {"mode": "hook"},
                        "tool_runtime": {"mode": "hook", "handler": "on_tool_runtime"},
                        "health": {"mode": "hook"},
                        "tools": {"mode": "declared", "exports": []},
                        "api": {"mode": "declared", "exports": []},
                        "events": {"mode": "declared", "exports": []},
                        "ingest_triggers": {"mode": "declared", "exports": []},
                        "auth_requirements": {"mode": "declared", "exports": []},
                        "migrations": {"mode": "declared", "exports": []},
                        "notifications": {"mode": "declared", "exports": []},
                    },
                    "supports_multi_user": True,
                    "supports_policy_metadata": True,
                    "supports_redaction": True,
                },
            }))
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({
                "plugins": {
                    "enabled": True,
                    "strict": False,
                    "apiVersion": 1,
                    "paths": ["plugins", "vendor/plugins"],
                    "allowList": ["openclaw.adapter", "core.extract", "memorydb.core"],
                    "slots": {
                        "adapter": "openclaw.adapter",
                        "ingest": ["core.extract"],
                        "dataStores": ["memorydb.core"]
                    }
                }
            }))
            with patch.object(config, "_config_paths", lambda: [config_file]), \
                    patch.object(config, "_workspace_root", lambda: tmp_path):
                cfg = load_config()
                assert cfg.plugins.enabled is True
                assert cfg.plugins.strict is False
                assert cfg.plugins.api_version == 1
                assert cfg.plugins.paths == ["plugins", "vendor/plugins"]
                assert cfg.plugins.allowlist == ["openclaw.adapter", "core.extract", "memorydb.core"]
                assert cfg.plugins.slots.adapter == "openclaw.adapter"
                assert cfg.plugins.slots.ingest == ["core.extract"]
                assert cfg.plugins.slots.datastores == ["memorydb.core"]
        finally:
            config._config = old_config

    def test_loads_janitor_run_tests_from_config(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({"janitor": {"runTests": True}}))
            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                assert cfg.janitor.run_tests is True
        finally:
            config._config = old_config

    def test_loads_project_definitions_from_db_with_list_type_validation(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            db_path = tmp_path / "data" / "memory.db"
            db_path.parent.mkdir(parents=True, exist_ok=True)
            conn = sqlite3.connect(db_path)
            conn.execute(
                """
                CREATE TABLE project_definitions (
                    name TEXT PRIMARY KEY,
                    label TEXT,
                    home_dir TEXT,
                    source_roots TEXT,
                    auto_index INTEGER,
                    patterns TEXT,
                    exclude TEXT,
                    description TEXT,
                    state TEXT
                )
                """
            )
            conn.execute(
                """
                INSERT INTO project_definitions
                (name, label, home_dir, source_roots, auto_index, patterns, exclude, description, state)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    "db-proj",
                    "DB Project",
                    "projects/db-proj/",
                    '{"bad":"type"}',
                    1,
                    '{"bad":"type"}',
                    '{"bad":"type"}',
                    "from db",
                    "active",
                ),
            )
            conn.commit()
            conn.close()

            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({}))

            with patch.object(config, "_config_paths", lambda: [config_file]), \
                 patch("lib.config.get_db_path", return_value=db_path):
                cfg = load_config()

            loaded = cfg.projects.definitions["db-proj"]
            assert loaded.source_roots == []
            assert loaded.patterns == ["*.md"]
            assert loaded.exclude == []
        finally:
            config._config = old_config

    def test_project_definition_json_fallback_validates_list_types(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({
                "projects": {
                    "definitions": {
                        "json-proj": {
                            "label": "JSON Project",
                            "homeDir": "projects/json-proj/",
                            "sourceRoots": {"bad": "type"},
                            "autoIndex": True,
                            "patterns": {"bad": "type"},
                            "exclude": "not-a-list",
                            "description": "from json",
                        }
                    }
                }
            }))

            with patch.object(config, "_config_paths", lambda: [config_file]), \
                 patch("lib.config.get_db_path", return_value=tmp_path / "missing.db"):
                cfg = load_config()

            loaded = cfg.projects.definitions["json-proj"]
            assert loaded.source_roots == []
            assert loaded.patterns == ["*.md"]
            assert loaded.exclude == ["*.db", "*.log", "*.pyc", "__pycache__/"]
        finally:
            config._config = old_config

    def test_core_parallel_llm_workers_default_is_4(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({"core": {"parallel": {"enabled": True}}}))
            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                assert cfg.core.parallel.llm_workers == 4
        finally:
            config._config = old_config

    def test_invalid_parallel_numeric_values_fall_back_to_defaults(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({
                "core": {
                    "parallel": {
                        "llmWorkers": "four",
                        "lifecyclePrepassWorkers": "three",
                        "lockWaitSeconds": "sixty",
                    }
                }
            }))
            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                assert cfg.core.parallel.llm_workers == 4
                assert cfg.core.parallel.lifecycle_prepass_workers == 3
                assert cfg.core.parallel.lock_wait_seconds == 120
        finally:
            config._config = old_config

    def test_invalid_model_numeric_values_fall_back_to_defaults(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({
                "models": {
                    "fastReasoningContext": "huge",
                    "deepReasoningContext": "massive",
                    "fastReasoningMaxOutput": "many",
                    "deepReasoningMaxOutput": "lots",
                    "batchBudgetPercent": "half",
                }
            }))
            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                assert cfg.models.fast_reasoning_context == 200000
                assert cfg.models.deep_reasoning_context == 200000
                assert cfg.models.fast_reasoning_max_output == 8192
                assert cfg.models.deep_reasoning_max_output == 16384
                assert cfg.models.batch_budget_percent == 0.50
        finally:
            config._config = old_config

    def test_invalid_token_budget_falls_back_to_zero(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({"janitor": {"tokenBudget": "many"}}))
            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                assert cfg.janitor.token_budget == 0
        finally:
            config._config = old_config

    def test_invalid_json_uses_defaults(self, tmp_path):
        """Invalid JSON in config file falls back to defaults."""
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text("not valid json {{{")

            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                assert isinstance(cfg, MemoryConfig)
                # Defaults should be used
                assert cfg.decay.threshold_days == 30
        finally:
            config._config = old_config

    def test_memorydb_plugin_config_hook_syncs_registry_and_tools(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            db_path = tmp_path / "data" / "memory.db"
            db_path.parent.mkdir(parents=True, exist_ok=True)
            conn = sqlite3.connect(db_path)
            conn.execute("CREATE TABLE IF NOT EXISTS nodes (id TEXT PRIMARY KEY)")
            conn.commit()
            conn.close()
            (tmp_path / "plugins" / "memorydb").mkdir(parents=True)
            (tmp_path / "plugins" / "memorydb" / "plugin.json").write_text(json.dumps({
                "plugin_api_version": 1,
                "plugin_id": "memorydb.core",
                "plugin_type": "datastore",
                "module": "core.plugins.memorydb_contract",
                "capabilities": {
                    "display_name": "MemoryDB",
                    "contract": {
                        "init": {"mode": "hook", "handler": "on_init"},
                        "config": {"mode": "hook", "handler": "on_config"},
                        "status": {"mode": "hook", "handler": "on_status"},
                        "dashboard": {"mode": "hook", "handler": "on_dashboard"},
                        "maintenance": {"mode": "hook", "handler": "on_maintenance"},
                        "tool_runtime": {"mode": "hook", "handler": "on_tool_runtime"},
                        "health": {"mode": "hook", "handler": "on_health"},
                        "tools": {"mode": "declared", "exports": []},
                        "api": {"mode": "declared", "exports": []},
                        "events": {"mode": "declared", "exports": []},
                        "ingest_triggers": {"mode": "declared", "exports": []},
                        "auth_requirements": {"mode": "declared", "exports": []},
                        "migrations": {"mode": "declared", "exports": []},
                        "notifications": {"mode": "declared", "exports": []},
                    },
                    "supports_multi_user": True,
                    "supports_policy_metadata": True,
                    "supports_redaction": True,
                },
            }))

            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({
                "plugins": {
                    "enabled": True,
                    "strict": True,
                    "apiVersion": 1,
                    "paths": ["plugins"],
                    "allowList": ["memorydb.core"],
                    "slots": {"dataStores": ["memorydb.core"]},
                },
                "retrieval": {
                    "domains": {
                        "technical": "code and systems",
                        "research": "comparisons and tradeoffs",
                    }
                }
            }))
            called = {"value": False}

            def _capture_sync(*args, **kwargs):
                called["value"] = True
                return False

            with patch.dict(os.environ, {"MEMORY_DB_PATH": str(db_path)}, clear=False), \
                 patch.object(config, "_config_paths", lambda: [config_file]), \
                 patch.object(config, "_workspace_root", lambda: tmp_path), \
                 patch("core.plugins.memorydb_contract.sync_tools_domain_block", side_effect=_capture_sync):
                _ = load_config()

            with sqlite3.connect(db_path) as verify:
                rows = verify.execute(
                    "SELECT domain, active FROM domain_registry ORDER BY domain"
                ).fetchall()
                assert ("research", 1) in rows
                assert ("technical", 1) in rows
                verify.execute("SELECT 1 FROM node_domains LIMIT 1").fetchall()
            assert called["value"] is True
        finally:
            config._config = old_config

    def test_memorydb_plugin_contract_creates_missing_db_parent_directory(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            db_path = tmp_path / "nested" / "deep" / "memory.db"
            assert not db_path.parent.exists()

            (tmp_path / "plugins" / "memorydb").mkdir(parents=True)
            (tmp_path / "plugins" / "memorydb" / "plugin.json").write_text(json.dumps({
                "plugin_api_version": 1,
                "plugin_id": "memorydb.core",
                "plugin_type": "datastore",
                "module": "core.plugins.memorydb_contract",
                "capabilities": {
                    "display_name": "MemoryDB",
                    "contract": {
                        "init": {"mode": "hook", "handler": "on_init"},
                        "config": {"mode": "hook", "handler": "on_config"},
                        "status": {"mode": "hook", "handler": "on_status"},
                        "dashboard": {"mode": "hook", "handler": "on_dashboard"},
                        "maintenance": {"mode": "hook", "handler": "on_maintenance"},
                        "tool_runtime": {"mode": "hook", "handler": "on_tool_runtime"},
                        "health": {"mode": "hook", "handler": "on_health"},
                        "tools": {"mode": "declared", "exports": []},
                        "api": {"mode": "declared", "exports": []},
                        "events": {"mode": "declared", "exports": []},
                        "ingest_triggers": {"mode": "declared", "exports": []},
                        "auth_requirements": {"mode": "declared", "exports": []},
                        "migrations": {"mode": "declared", "exports": []},
                        "notifications": {"mode": "declared", "exports": []},
                    },
                    "supports_multi_user": True,
                    "supports_policy_metadata": True,
                    "supports_redaction": True,
                },
            }))
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({
                "plugins": {
                    "enabled": True,
                    "strict": True,
                    "apiVersion": 1,
                    "paths": ["plugins"],
                    "allowList": ["memorydb.core"],
                    "slots": {"dataStores": ["memorydb.core"]},
                }
            }))
            with patch.dict(os.environ, {"MEMORY_DB_PATH": str(db_path)}, clear=False), \
                 patch.object(config, "_config_paths", lambda: [config_file]), \
                 patch.object(config, "_workspace_root", lambda: tmp_path):
                _ = load_config()

            assert db_path.parent.exists()
            with sqlite3.connect(db_path) as verify:
                verify.execute("SELECT COUNT(*) FROM domain_registry").fetchone()
        finally:
            config._config = old_config

    def test_memorydb_contract_on_status_returns_zero_if_domain_table_missing(self, tmp_path):
        from core.plugins.memorydb_contract import MemoryDbPluginContract
        from core.runtime.plugins import PluginHookContext, PluginManifest

        contract = MemoryDbPluginContract()
        ctx = PluginHookContext(
            plugin=PluginManifest(
                plugin_api_version=1,
                plugin_id="memorydb.core",
                plugin_type="datastore",
                module="core.plugins.memorydb_contract",
                display_name="MemoryDB",
            ),
            config=MemoryConfig(),
            plugin_config={},
            workspace_root=str(tmp_path),
        )

        status = contract.on_status(ctx)
        assert status["active_domains"] == 0

    def test_invalid_adapter_slot_fails_contract_validation(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({
                "plugins": {
                    "slots": {
                        "adapter": "bad adapter id",
                    }
                }
            }))
            with patch.object(config, "_config_paths", lambda: [config_file]):
                with pytest.raises(ValueError, match="Invalid plugin id"):
                    _ = load_config()
        finally:
            config._config = old_config

    def test_plugin_config_keys_preserved_as_opaque_payload(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({
                "plugins": {
                    "enabled": False,
                    "config": {
                        "memorydb.core": {
                            "customKey": True,
                            "nestedMap": {"innerKey": 1},
                        }
                    },
                }
            }))
            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                payload = cfg.plugins.config.get("memorydb.core", {})
                assert "customKey" in payload
                assert "nestedMap" in payload
                assert "custom_key" not in payload
                assert "nested_map" not in payload
        finally:
            config._config = old_config

    def test_models_base_url_and_api_key_env_loaded_from_config(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({
                "models": {
                    "llmProvider": "openai-compatible",
                    "apiKeyEnv": "E2E_TEST_KEY_OPENAI",
                    "baseUrl": "https://openrouter.ai/api/v1",
                }
            }))
            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                assert cfg.models.llm_provider == "openai-compatible"
                assert cfg.models.api_key_env == "E2E_TEST_KEY_OPENAI"
                assert cfg.models.base_url == "https://openrouter.ai/api/v1"
        finally:
            config._config = old_config

    def test_users_identity_keys_preserve_user_id_casing(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({
                "users": {
                    "defaultOwner": "solomonSteadman",
                    "identities": {
                        "solomonSteadman": {
                            "channels": {"cli": ["*"]},
                            "speakers": ["Solomon"],
                            "personNodeName": "Solomon",
                        }
                    },
                }
            }))
            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                assert "solomonSteadman" in cfg.users.identities
                assert "solomon_steadman" not in cfg.users.identities
        finally:
            config._config = old_config

    def test_retrieval_router_fail_open_respects_config(self, tmp_path):
        import config
        old_config = config._config
        config._config = None
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({
                "retrieval": {
                    "routerFailOpen": False
                }
            }))
            with patch.object(config, "_config_paths", lambda: [config_file]):
                cfg = load_config()
                assert cfg.retrieval.router_fail_open is False
        finally:
            config._config = old_config

    def test_reload_config_resets_unknown_key_warning_cache(self, tmp_path, capsys):
        import config
        old_config = config._config
        old_warned = set(config._warned_unknown_config_keys)
        config._config = None
        config._warned_unknown_config_keys.clear()
        try:
            config_file = tmp_path / "memory.json"
            config_file.write_text(json.dumps({
                "totallyUnknownSection": {"enabled": True},
            }))
            with patch.object(config, "_config_paths", lambda: [config_file]), \
                 patch.dict(os.environ, {"QUAID_QUIET": ""}, clear=False):
                _ = load_config()
                first_err = capsys.readouterr().err
                _ = reload_config()
                second_err = capsys.readouterr().err
            assert "Unknown config key ignored: totally_unknown_section" in first_err
            assert "Unknown config key ignored: totally_unknown_section" in second_err
        finally:
            config._config = old_config
            config._warned_unknown_config_keys.clear()
            config._warned_unknown_config_keys.update(old_warned)
